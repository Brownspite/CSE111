{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownspite/CSE111/blob/main/model_of_422.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
        "from seaborn import heatmap\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import plotly.express as px\n",
        "\n",
        "dataset = pd.read_csv(\"/content/software_quality_dataset.csv\")\n",
        "dataset.shape\n",
        "\n",
        "dataset.head()\n",
        "\n",
        "dataset.info()\n",
        "\n",
        "dataset.isnull().sum()\n",
        "\n",
        "feature_data_types = {}\n",
        "for col in dataset.columns:\n",
        "    if col == 'Quality_Label':\n",
        "        continue\n",
        "    unique_values = dataset[col].unique()\n",
        "    if pd.api.types.is_numeric_dtype(dataset[col]):\n",
        "        feature_data_types[col] = \"Quantitative\"\n",
        "    else:\n",
        "        feature_data_types[col] = \"Categorical\"\n",
        "\n",
        "target_variable_unique_values = dataset['Quality_Label'].unique() # Changed df to dataset\n",
        "print(f\"\\nFeature Data Types:\\n{feature_data_types}\")\n",
        "print(f\"\\nUnique values of the target variable 'Quality_Label': {target_variable_unique_values}\")\n",
        "\n",
        "# Encode the categorical features\n",
        "le = LabelEncoder()\n",
        "dataset['Has_Unit_Tests'] = le.fit_transform(dataset['Has_Unit_Tests'])\n",
        "dataset['Quality_Label'] = le.fit_transform(dataset['Quality_Label'])\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = dataset.corr()\n",
        "\n",
        "# Visualize the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Features')\n",
        "plt.show()\n",
        "# Analyze the correlation results\n",
        "print(\"Correlation with Quality_Label:\")\n",
        "print(correlation_matrix['Quality_Label'].sort_values(ascending=False))\n",
        "\n",
        "# Analyze the target variable for class imbalance\n",
        "quality_counts = dataset['Quality_Label'].value_counts()\n",
        "print(\"\\nQuality Label Counts:\")\n",
        "print(quality_counts)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "quality_counts.plot(kind='bar', color=['skyblue', 'salmon', 'lightgreen'])\n",
        "plt.title('Distribution of Quality Labels')\n",
        "plt.xlabel('Quality Label')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n",
        "\n",
        "##Selecting numerical features\n",
        "numerical_data = dataset.select_dtypes(include='number')\n",
        "\n",
        "#append the features of numerical_data to list\n",
        "numerical_features=numerical_data.columns.tolist()\n",
        "\n",
        "print(f'There are {len(numerical_features)} numerical features:', '\\n')\n",
        "print(numerical_features)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Selecting categoricalfeatures\n",
        "categorical_data=dataset.select_dtypes(include= 'object')\n",
        "\n",
        "#append the features of categorical_data to list\n",
        "categorical_features=categorical_data.columns.tolist()\n",
        "\n",
        "print(f'There are {len(categorical_features)} numerical features:', '\\n')\n",
        "print(categorical_features)\n",
        "\n",
        "numerical_data.nunique()\n",
        "\n",
        "# Impute missing values using the median for numerical features\n",
        "for col in ['Lines_of_Code', 'Code_Churn', 'Comment_Density']:\n",
        "    dataset[col].fillna(dataset[col].median(), inplace=True) # Changed df to dataset\n",
        "\n",
        "# Check if 'Has_Unit_Tests' column exists before one-hot encoding\n",
        "if 'Has_Unit_Tests' in dataset.columns:\n",
        "    # One-hot encode the 'Has_Unit_Tests' column if it exists\n",
        "    dataset = pd.get_dummies(dataset, columns=['Has_Unit_Tests'], prefix='Has_Unit_Tests') # Changed df to dataset\n",
        "\n",
        "# Verify data cleaning\n",
        "print(\"Number of missing values after imputation:\")\n",
        "print(dataset.isnull().sum()) # Changed df to dataset\n",
        "\n",
        "print(\"\\nFirst few rows of the cleaned data:\")\n",
        "display(dataset.head()) # Changed df to dataset\n",
        "\n",
        "dataset.isnull().sum()\n",
        "\n",
        "dataset = pd.get_dummies(dataset, columns=['Has_Unit_Tests'], prefix='Has_Unit_Tests')\n",
        "\n",
        "scaler= StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "quality_counts = dataset['Quality_Label'].value_counts()\n",
        "print(\"\\nQuality Label Counts:\")\n",
        "print(quality_counts)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "quality_counts.plot(kind='bar', color=['skyblue', 'salmon', 'lightgreen'])\n",
        "plt.title('Distribution of Quality Labels')\n",
        "plt.xlabel('Quality Label')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPercentage of each class:\")\n",
        "print(quality_counts / len(dataset) * 100)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming your preprocessed dataset is stored in a pandas DataFrame called 'dataset'\n",
        "X = dataset.drop('Quality_Label', axis=1)  # Features\n",
        "y = dataset['Quality_Label']  # Target variable\n",
        "\n",
        "# Stratified splitting with 70% for training and 30% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "# Create and train the KNN model with increased n_neighbors\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)  # Increase n_neighbors to 5 (or another value)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "knn_pred = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "knn_training_accuracy = accuracy_score(y_train, knn_model.predict(X_train))\n",
        "knn_test_accuracy = accuracy_score(y_test, knn_pred)\n",
        "\n",
        "print(f'KNN Training Accuracy: {knn_training_accuracy}')\n",
        "print(f'KNN Testing Accuracy: {knn_test_accuracy}')\n",
        "\n",
        "# Get precision, recall, and F1-score\n",
        "report = classification_report(y_test, knn_pred)\n",
        "print(report)\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(y_test, knn_pred, pos_label=1)\n",
        "knn_precision = precision[1]\n",
        "knn_recall = recall[1]\n",
        "\n",
        "print('KNN Precision:', knn_precision)\n",
        "print('KNN Recall:', knn_recall)\n",
        "\n",
        "# Create and train the Decision Tree model\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "dt_pred = decision_tree_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "dt_training_accuracy = accuracy_score(y_train, decision_tree_model.predict(X_train))\n",
        "dt_test_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "print(f'Decision Tree Training Accuracy: {dt_training_accuracy}')\n",
        "print(f'Decision Tree Testing Accuracy: {dt_test_accuracy}')\n",
        "\n",
        "# Get precision, recall, and F1-score\n",
        "report = classification_report(y_test, dt_pred)\n",
        "print(report)\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(y_test, dt_pred, pos_label=1)\n",
        "dt_precision = precision[1]\n",
        "dt_recall = recall[1]\n",
        "\n",
        "print('Decision Tree Precision:', dt_precision)\n",
        "print('Decision Tree Recall:', dt_recall)\n",
        "\n",
        "# Define the neural network model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer with 64 units and ReLU activation\n",
        "    keras.layers.Dense(32, activation='relu'),  # Hidden layer with 32 units and ReLU activation\n",
        "    keras.layers.Dense(1, activation='sigmoid')  # Output layer with 1 unit and sigmoid activation (for binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  # Use binary_crossentropy for binary classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)  # Adjust epochs and batch_size as needed\n",
        "\n",
        "# Make predictions on the test set\n",
        "nn_pred_probs = model.predict(X_test)\n",
        "nn_pred = (nn_pred_probs > 0.5).astype(int)  # Convert probabilities to class labels (0 or 1)\n",
        "\n",
        "# Evaluate the model\n",
        "nn_training_accuracy = accuracy_score(y_train, (model.predict(X_train) > 0.5).astype(int))\n",
        "nn_test_accuracy = accuracy_score(y_test, nn_pred)\n",
        "\n",
        "print(f'Neural Network Training Accuracy: {nn_training_accuracy}')\n",
        "print(f'Neural Network Testing Accuracy: {nn_test_accuracy}')\n",
        "\n",
        "# Get precision, recall, and F1-score\n",
        "report = classification_report(y_test, nn_pred)\n",
        "print(report)\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(y_test, nn_pred, pos_label=1)\n",
        "nn_precision = precision[1]\n",
        "nn_recall = recall[1]\n",
        "\n",
        "print('Neural Network Precision:', nn_precision)\n",
        "print('Neural Network Recall:', nn_recall)\n",
        "\n",
        "models = ['KNN', 'Decision Tree', 'Neural Network']\n",
        "accuracies = [knn_test_accuracy, dt_test_accuracy, nn_test_accuracy]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(models, accuracies, color=['blue', 'green', 'red'])\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)  # Set y-axis limits for better visualization\n",
        "plt.show()\n",
        "\n",
        "# 2. Precision-Recall Comparison\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(models, [knn_precision, dt_precision, nn_precision], color=['blue', 'green', 'red'], label='Precision')\n",
        "plt.bar(models, [knn_recall, dt_recall, nn_recall], color=['lightblue', 'lightgreen', 'lightcoral'], label='Recall', alpha=0.7)  # Added alpha for transparency\n",
        "plt.title('Precision-Recall Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "knn_cm = confusion_matrix(y_test, knn_pred)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(knn_cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('KNN Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Decision Tree Confusion Matrix\n",
        "dt_cm = confusion_matrix(y_test, dt_pred)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(dt_cm, annot=True, fmt='d', cmap='Greens', cbar=False)\n",
        "plt.title('Decision Tree Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Neural Network Confusion Matrix\n",
        "nn_cm = confusion_matrix(y_test, nn_pred)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(nn_cm, annot=True, fmt='d', cmap='Reds', cbar=False)\n",
        "plt.title('Neural Network Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Predict using each model\n",
        "knn_pred = knn_model.predict(X_test)\n",
        "dt_pred = decision_tree_model.predict(X_test)\n",
        "nn_pred = np.argmax(model.predict(X_test), axis=1) # Changed neural_network_model to model\n",
        "\n",
        "# Assuming y_test is your original target variable\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_test_encoded = le.fit_transform(y_test)  # Encode y_test if it's not already encoded\n",
        "\n",
        "\n",
        "# Function to evaluate a model and print results\n",
        "def evaluate_model(model_name, y_true, y_pred):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    print(f\"--- {model_name} ---\")\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "\n",
        "# Evaluate each model\n",
        "evaluate_model(\"KNN\", y_test_encoded, knn_pred)\n",
        "evaluate_model(\"Decision Tree\", y_test_encoded, dt_pred)\n",
        "evaluate_model(\"Neural Network\", y_test_encoded, nn_pred)\n",
        "\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "!pip install imblearn\n",
        "from imblearn.under_sampling import RandomUnderSampler # Import RandomUnderSampler\n",
        "# --- Undersampling ---\n",
        "# Instantiate RandomUnderSampler\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "\n",
        "# Resample the data\n",
        "X_undersampled, y_undersampled = undersampler.fit_resample(X, y)\n",
        "\n",
        "# --- Oversampling (SMOTE) ---\n",
        "# Instantiate SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Resample the data\n",
        "X_oversampled, y_oversampled = smote.fit_resample(X, y)\n",
        "\n",
        "\n",
        "# --- Model Training and Evaluation (for each sampling technique) ---\n",
        "def train_and_evaluate_model(X_train, y_train, X_test, y_test, model_name, model):\n",
        "    \"\"\"Trains and evaluates a given model.\"\"\"\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Get predicted probabilities for Neural Network\n",
        "    if model_name == \"Neural Network\":\n",
        "        y_pred_probs = model.predict(X_test)\n",
        "        y_pred = (y_pred_probs > 0.5).astype(int)  # Convert probabilities to class labels (0 or 1)\n",
        "    else:\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    precision, recall, _, _ = precision_recall_fscore_support(y_test, y_pred, pos_label=1)\n",
        "\n",
        "    print(f\"--- {model_name} ---\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision[1]:.4f}\")\n",
        "    print(f\"Recall: {recall[1]:.4f}\")\n",
        "    print(report)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return accuracy, precision[1], recall[1]  # Return metrics for comparison\n",
        "\n",
        "# --- Original Data ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# --- Undersampled Data ---\n",
        "X_train_under, X_test_under, y_train_under, y_test_under = train_test_split(\n",
        "    X_undersampled, y_undersampled, test_size=0.3, random_state=42, stratify=y_undersampled\n",
        ")\n",
        "X_train_under = scaler.fit_transform(X_train_under)\n",
        "X_test_under = scaler.transform(X_test_under)\n",
        "\n",
        "\n",
        "# --- Oversampled Data ---\n",
        "X_train_over, X_test_over, y_train_over, y_test_over = train_test_split(\n",
        "    X_oversampled, y_oversampled, test_size=0.3, random_state=42, stratify=y_oversampled\n",
        ")\n",
        "X_train_over = scaler.fit_transform(X_train_over)\n",
        "X_test_over = scaler.transform(X_test_over)\n",
        "\n",
        "# --- Model Initialization ---\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "neural_network_model = keras.Sequential([\n",
        "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "neural_network_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# --- Model Training and Evaluation ---\n",
        "print(\"Original Data:\")\n",
        "original_results = {}\n",
        "original_results['KNN'] = train_and_evaluate_model(X_train, y_train, X_test, y_test, \"KNN\", knn_model)\n",
        "original_results['Decision Tree'] = train_and_evaluate_model(X_train, y_train, X_test, y_test, \"Decision Tree\", decision_tree_model)\n",
        "original_results['Neural Network'] = train_and_evaluate_model(X_train, y_train, X_test, y_test, \"Neural Network\", neural_network_model)\n",
        "\n",
        "print(\"\\nUndersampled Data:\")\n",
        "undersampled_results = {}\n",
        "undersampled_results['KNN'] = train_and_evaluate_model(X_train_under, y_train_under, X_test_under, y_test_under, \"KNN\", knn_model)\n",
        "undersampled_results['Decision Tree'] = train_and_evaluate_model(X_train_under, y_train_under, X_test_under, y_test_under, \"Decision Tree\", decision_tree_model)\n",
        "undersampled_results['Neural Network'] = train_and_evaluate_model(X_train_under, y_train_under, X_test_under, y_test_under, \"Neural Network\", neural_network_model)\n",
        "\n",
        "\n",
        "print(\"\\nOversampled Data:\")\n",
        "oversampled_results = {}\n",
        "oversampled_results['KNN'] = train_and_evaluate_model(X_train_over, y_train_over, X_test_over, y_test_over, \"KNN\", knn_model)\n",
        "oversampled_results['Decision Tree'] = train_and_evaluate_model(X_train_over, y_train_over, X_test_over, y_test_over, \"Decision Tree\", decision_tree_model)\n",
        "oversampled_results['Neural Network'] = train_and_evaluate_model(X_train_over, y_train_over, X_test_over, y_test_over, \"Neural Network\", neural_network_model)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have the original_results, undersampled_results, and oversampled_results dictionaries\n",
        "\n",
        "# Extract accuracies for each model and sampling technique\n",
        "knn_accuracies = [original_results['KNN'][0], undersampled_results['KNN'][0], oversampled_results['KNN'][0]]\n",
        "dt_accuracies = [original_results['Decision Tree'][0], undersampled_results['Decision Tree'][0], oversampled_results['Decision Tree'][0]]\n",
        "nn_accuracies = [original_results['Neural Network'][0], undersampled_results['Neural Network'][0], oversampled_results['Neural Network'][0]]\n",
        "\n",
        "# Set up the bar chart\n",
        "models = ['KNN', 'Decision Tree', 'Neural Network']\n",
        "sampling_techniques = ['Original', 'Undersampled', 'Oversampled']\n",
        "x = np.arange(len(models))  # the label locations\n",
        "width = 0.2  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "rects1 = ax.bar(x - width, knn_accuracies, width, label='KNN')\n",
        "rects2 = ax.bar(x, dt_accuracies, width, label='Decision Tree')\n",
        "rects3 = ax.bar(x + width, nn_accuracies, width, label='Neural Network')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Accuracy Comparison across Sampling Techniques')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "# Add value labels to the bars\n",
        "def autolabel(rects):\n",
        "    \"\"\"Attach a text label above each bar in rects, displaying its height.\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate('{:.2f}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "autolabel(rects3)\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "pd.Series(y_resampled).value_counts()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "\n",
        "# ... (previous code for model training and evaluation) ...\n",
        "\n",
        "# 1. Bar chart for accuracy\n",
        "models = ['KNN', 'Decision Tree', 'Neural Network']\n",
        "accuracies = [knn_test_accuracy, dt_test_accuracy, nn_test_accuracy]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(models, accuracies, color=['blue', 'green', 'red'])\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)  # Set y-axis limits for better visualization\n",
        "plt.show()\n",
        "\n",
        "# 2. Precision-Recall Comparison\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(models, [knn_precision, dt_precision, nn_precision], color=['blue', 'green', 'red'], label='Precision')\n",
        "plt.bar(models, [knn_recall, dt_recall, nn_recall], color=['lightblue', 'lightgreen', 'lightcoral'], label='Recall', alpha=0.7)  # Added alpha for transparency\n",
        "plt.title('Precision-Recall Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 3. Confusion Matrices\n",
        "# KNN Confusion Matrix\n",
        "knn_cm = confusion_matrix(y_test, knn_pred)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(knn_cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('KNN Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Decision Tree Confusion Matrix\n",
        "dt_cm = confusion_matrix(y_test, dt_pred)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(dt_cm, annot=True, fmt='d', cmap='Greens', cbar=False)\n",
        "plt.title('Decision Tree Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Neural Network Confusion Matrix\n",
        "nn_cm = confusion_matrix(y_test, nn_pred)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(nn_cm, annot=True, fmt='d', cmap='Reds', cbar=False)\n",
        "plt.title('Neural Network Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3QnMgd3vo-Cc",
        "outputId": "8d4eb8da-9a08-4841-e5ae-db960b136d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8ad73739af6d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtseries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/api/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" public toolkit API \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from pandas.api import (\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mindexers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minterchange\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/api/typing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# TODO: Can't import Styler without importing jinja2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# from pandas.io.formats.style import Styler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJsonReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStataReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.io.json._json import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mread_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mto_json\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mujson_dumps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mujson_loads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mparse_table_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m )\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.io.parsers.readers import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mTextFileReader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mTextParser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mread_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mread_fwf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTR_NA_VALUES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m from pandas.errors import (\n\u001b[1;32m     34\u001b[0m     \u001b[0mAbstractMethodError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36minit pandas._libs.parsers\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}